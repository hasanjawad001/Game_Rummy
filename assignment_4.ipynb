{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> jawad chowdhury </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Overview\n",
    "\n",
    "<!-- Describe the objective of this assignment. You can briefly state how you accompilsh it. -->\n",
    "\n",
    "The main objective of this assignment was to implement a modified version of Game Rummy by training an agent with Re-inforcement learning.\n",
    "\n",
    "Here, i get familiarized with the basic Reinforcement environment including states, actions, rewards etc.\n",
    "\n",
    "And also got some idea about different approaches such as SAARSA, Q-learning and some related methodologies like epsilon-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rummy Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the SUITS, RANKS of the cards and their RANK_VALUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUIT = ['H','S','D','C']\n",
    "RANK = ['A', '2', '3', '4', '5','6','7']\n",
    "RANK_VALUE = {'A': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'T': 10, 'Q': 10, 'K': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card Class Definition\n",
    "<!-- __init__  : Defines the card details such as rank, suit and calculates the rank value -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, rank, suit):\n",
    "        self.rank = rank\n",
    "        self.suit = suit\n",
    "        self.rank_to_val = RANK_VALUE[self.rank]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.rank}{self.suit}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.rank}{self.suit}'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.rank == other.rank and self.suit == other.suit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deck Class Definition\n",
    "<!-- __shuffle__ : Shuffles the deck in random order -->\n",
    "\n",
    "<!-- __draw_card__ : Draws a card from the top of the deck -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Deck:\n",
    "    def __init__(self, packs):\n",
    "        self.packs = packs\n",
    "        self.cards = []\n",
    "        for pack in range(0, packs):\n",
    "            for suit in SUIT:\n",
    "                for rank in RANK:\n",
    "                    self.cards.append(Card(rank, suit))\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "\n",
    "    def draw_card(self):\n",
    "        card = self.cards[0]\n",
    "        self.cards.pop(0)\n",
    "        return card\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class:\n",
    "<!-- \n",
    "### 1.__init__(self,name,stash=list(),isBot=False): \n",
    "Initializing stash, name, isBot/dealer points for each player.\n",
    "\n",
    "### 2. deal_card(self,card):\n",
    "This method appends the card in the stash and check the condition that length of stash should not be greater than nuber of cards length in game.\n",
    "\n",
    "### 3. drop_card(self,card):\n",
    "This method removes the card from stash and add that card into pile.\n",
    "\n",
    "### 4. meld(self):\n",
    "This method tries to find the cards with the same rank in the hand. If it finds then it will merge the cards in the hand to the melded cards array in the game. \n",
    "\n",
    "### 5. stash_score(self):\n",
    "This method calculates sum of all the cards in stash according to the rank of each card.\n",
    "\n",
    "### 6. get_info(self,debug):\n",
    "This function fetch all the information of the player. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Player:\n",
    "    \"\"\"\n",
    "        Player class to create a player object.\n",
    "        eg: player = Player(\"player1\", list(), isBot = False)\n",
    "        Above declaration will be for your agent.\n",
    "        All the player names should be unique or else you will get error.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, stash=list(), isBot=False, points=0, conn=None):\n",
    "        self.stash = stash\n",
    "        self.name = name\n",
    "        self.game = None\n",
    "        self.isBot = isBot\n",
    "        self.points = points\n",
    "        self.conn = conn\n",
    "\n",
    "    def deal_card(self, card):\n",
    "        try:\n",
    "            self.stash.append(card)\n",
    "            if len(self.stash) > self.game.cardsLength + 1:\n",
    "                raise ValueError('Cannot have cards greater than ')\n",
    "        except ValueError as err:\n",
    "            print(err.args)\n",
    "\n",
    "    def drop_card(self, card):\n",
    "        self.stash.remove(card)\n",
    "        self.game.add_pile(card)\n",
    "        return -1\n",
    "\n",
    "    def meld(self):\n",
    "        card_hash = defaultdict(list)\n",
    "        for card in self.stash:\n",
    "            card_hash[card.rank].append(card)\n",
    "        melded_card_ranks = []\n",
    "        for (card_rank, meld_cards) in card_hash.items():\n",
    "            if len(meld_cards) >= 3:\n",
    "                self.game.meld.append(meld_cards)\n",
    "                melded_card_ranks.append(card_rank)\n",
    "                for card in meld_cards:\n",
    "                    self.stash.remove(card)\n",
    "\n",
    "        for card_rank in melded_card_ranks:\n",
    "            card_hash.pop(card_rank)\n",
    "        return len(melded_card_ranks) > 0\n",
    "\n",
    "    def stash_score(self):\n",
    "        score = 0\n",
    "        for card in self.stash:\n",
    "            score += RANK_VALUE[card.rank]\n",
    "        return score\n",
    "\n",
    "    def get_info(self, debug):\n",
    "        if debug:\n",
    "            print(\n",
    "                f'Player Name : {self.name} \\n Stash Score: {self.stash_score()} \\n Stash : {\", \".join(str(x) for x in self.stash)}')\n",
    "        card_ranks = []\n",
    "        card_suits = []\n",
    "        pileset = None\n",
    "        pile = None\n",
    "        for card in self.stash:\n",
    "            card_ranks.append(RANK_VALUE[card.rank])\n",
    "            card_suits.append(card.suit)\n",
    "        if len(self.game.pile) > 0:\n",
    "            return {\n",
    "                \"Stash Score\": self.stash_score(),\n",
    "                \"CardSuit\": card_suits,\n",
    "                \"CardRanks\": card_ranks,\n",
    "                \"PileRank\": self.game.pile[-1].rank,\n",
    "                \"PileSuit\": self.game.pile[-1].suit\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"Stash Score\": self.stash_score(),\n",
    "            \"CardSuit\": card_suits,\n",
    "            \"CardRanks\": card_ranks\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RummyAgent():\n",
    "    \"\"\"\n",
    "    Simple Rummy Environment\n",
    "\n",
    "    Simple Rummy is a game where you need to make all the cards in your hand same before your opponent does.\n",
    "    Here you are given 3 cards in your hand/stash to play.\n",
    "    For the first move you have to pick a card from the deck or from the pile.\n",
    "    The card in deck would be random but you can see the card from the pile.\n",
    "    In the next move you will have to drop a card from your hand.\n",
    "    Your goal is to collect all the cards of the same rank.\n",
    "    Higher the rank of the card, the higher points you lose in the game.\n",
    "    You need to keep the stash score low. Eg, if you can AH,7S,5D your strategy would be to either find the first pair of the card or by removing the highest card in the deck.\n",
    "    You only have 20 turns to either win the same or collect low scoring card.\n",
    "    You can't see other players cards or their stash scores.\n",
    "\n",
    "    Parameters\n",
    "    ====\n",
    "    players: Player objects which will play the game.\n",
    "    max_card_length : Number of cards each player can have\n",
    "    max_turns: Number of turns in a rummy game\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, players, max_card_length=5, max_turns=20):\n",
    "        self.max_card_length = max_card_length\n",
    "        self.max_turns = max_turns\n",
    "        self.reset(players)\n",
    "\n",
    "    def update_player_cards(self, players):\n",
    "        for player in players:\n",
    "            player = Player(player.name, list(), isBot=player.isBot, points=player.points, conn=player.conn)\n",
    "            stash = []\n",
    "            for i in range(self.max_card_length):\n",
    "                player.stash.append(self.deck.draw_card())\n",
    "            player.game = self\n",
    "            self.players.append(player)\n",
    "        self.pile = [self.deck.draw_card()]\n",
    "\n",
    "    def add_pile(self, card):\n",
    "        if len(self.deck.cards) == 0:\n",
    "            self.deck.cards.extend(self.pile)\n",
    "            self.deck.shuffle()\n",
    "            self.pile = []\n",
    "        self.pile.append(card)\n",
    "\n",
    "    def pick_card(self, player, action):\n",
    "        before_unique_rank_list = list(set([card.rank_to_val for card in player.stash]))\n",
    "        before_unique_length = len(before_unique_rank_list)\n",
    "        ss_before = int(player.stash_score())\n",
    "        if action == 0:\n",
    "            self.pick_from_pile(player)\n",
    "        else:\n",
    "            self.pick_from_deck(player)\n",
    "        after_unique_rank_list = list(set([card.rank_to_val for card in player.stash]))\n",
    "        after_unique_length = len(after_unique_rank_list)\n",
    "        ss_after = int(player.stash_score())\n",
    "        ss_delta = ss_after - ss_before\n",
    "\n",
    "        s = [player.stash[0].rank_to_val, player.stash[1].rank_to_val, player.stash[2].rank_to_val, player.stash[3].rank_to_val]\n",
    "\n",
    "        if player.meld():\n",
    "            reward = 100\n",
    "        elif after_unique_length == before_unique_length:\n",
    "            reward = 90\n",
    "        else:\n",
    "            reward = -3 * ss_delta\n",
    "        return {\"reward\": reward, \"state\": s}\n",
    "\n",
    "\n",
    "\n",
    "    def pick_from_pile(self, player):\n",
    "        card = self.pile[-1]\n",
    "        self.pile.pop()\n",
    "        return player.stash.append(card)\n",
    "\n",
    "    def pick_from_deck(self, player):\n",
    "        return player.stash.append(self.deck.draw_card())\n",
    "\n",
    "    def get_player(self, player_name):\n",
    "        return_player = [player for player in self.players if player.name == player_name]\n",
    "        if len(return_player) != 1:\n",
    "            print(\"Invalid Player\")\n",
    "            return None\n",
    "        else:\n",
    "            return return_player[0]\n",
    "\n",
    "    def drop_card(self, player, card):\n",
    "        before_unique_rank_list = list(set([card.rank_to_val for card in player.stash]))\n",
    "        before_unique_length = len(before_unique_rank_list)\n",
    "        ss_before = int(player.stash_score())\n",
    "        player.drop_card(card)\n",
    "        after_unique_rank_list = list(set([card.rank_to_val for card in player.stash]))\n",
    "        after_unique_length = len(after_unique_rank_list)\n",
    "        ss_after = int(player.stash_score())\n",
    "        ss_delta = ss_after - ss_before\n",
    "\n",
    "        if before_unique_length  == after_unique_length:\n",
    "            reward = -90\n",
    "        else:\n",
    "            reward = -3 * ss_delta\n",
    "        return {\"reward\": reward}\n",
    "\n",
    "    def computer_play(self, player):\n",
    "        # Gets a card from deck or pile\n",
    "        if random.randint(0, 1) == 1:\n",
    "            self.pick_from_pile(player)\n",
    "        else:\n",
    "            self.pick_from_deck(player)\n",
    "\n",
    "        # tries to meld if it can\n",
    "        #         if random.randint(0,10) > 5 :\n",
    "        player.meld()\n",
    "\n",
    "        # removes a card from the stash\n",
    "        if len(player.stash) != 0:\n",
    "            card = player.stash[(random.randint(0, len(player.stash) - 1))]\n",
    "            player.drop_card(card)\n",
    "\n",
    "    def play(self):\n",
    "        for player in self.players:\n",
    "            if len(player.stash) == 0:\n",
    "                return True\n",
    "        if self.max_turns <= 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _update_turn(self):\n",
    "        self.max_turns -= 1\n",
    "\n",
    "    def reset(self, players, max_turns=20):\n",
    "        self.players = []\n",
    "        self.deck = Deck(1)\n",
    "        self.deck.shuffle()\n",
    "        self.meld = []\n",
    "        self.pile = []\n",
    "        self.max_turns = max_turns\n",
    "        self.update_player_cards(players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A: Review of the SARSA and Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different approaches to update the Q-function in TD learning. We can update the Q-function based on \n",
    "\n",
    "some policy or we can also update the Q-function without following any policy.\n",
    "\n",
    "**1. SARSA:**\n",
    "\n",
    "SARSA basically refers to an approach such as:\n",
    "\n",
    "**(S)tate -> (A)ction -> (R)eward - > (S)tate -> (A)ction**\n",
    "\n",
    "This is the on-policy approach of TD-learning where taking the action for next steps follows some certain \n",
    "\n",
    "behavior. According to this approach, we update our Q-function as follows:\n",
    "$$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha ( R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) \n",
    "$$\n",
    "\n",
    "\n",
    "**2. Q-Learning:**\n",
    "\n",
    "This is basically the off-policy approach, here for the control problem to update our Q-function we dont make \n",
    "\n",
    "any assumption of behavior policy. According to this approach, we update our Q-function as:\n",
    "$$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha ( R_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B: Choice of TD learning and Reason\n",
    "\n",
    "For my implementation in this assignment, i have chosen the **SARSA** approach to follow the TD-learning.\n",
    "\n",
    "\n",
    "Basically the 2 approaches differ mostly on the evaluation of $Q(s_{t+1}, a_{t+1})$ portion where in **SARSA** \n",
    "\n",
    "we take the next action $ a_{t+1}$ by epsilon-greedy approach and evaluate the value, based on the value of \n",
    "\n",
    "epsilon the probability differs whether this evaluation would be max or not. \n",
    "\n",
    "But in **Q-learning** we try to grab $Q(s_{t+1}, a_{t+1})$ with maximum value. So, my thought was that in \n",
    "\n",
    "**SARSA** using the epsilon i can have more control in the exploration-exploitation manner, though we use \n",
    "\n",
    "epsilon-greedy for the action in **Q-learning** too but for consideration of next Q(state, action) it takes the \n",
    "\n",
    "maximum always. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.C Choice of Function Approximation and Reason\n",
    "\n",
    "I used the Tabular function approximation Q and defined the states based on the different states of player's \n",
    "\n",
    "hand (diffenrent combination of cards) and the actions depending on the decision to make.\n",
    "\n",
    "Because it felt convenient for me to have the states and the actions well defined, i can also evaluate the \n",
    "\n",
    "Tabular values to make some sense about the approximation of different stage of training how the RL-agent is \n",
    "\n",
    "making the decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.D Implementation of the selected approach (RLAgent)\n",
    "\n",
    "#### RLAgent for Rummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coord_convert(s, sz):\n",
    "#     return [s[1], sz[0]-s[0]-1]\n",
    "\n",
    "class RLAgent:\n",
    "    \"\"\"\n",
    "        Reinforcement Learning Agent Model for training/testing\n",
    "        with Tabular function approximation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def printQ(self):\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                for k in range(7):\n",
    "                    for l in range(7):\n",
    "                        for m in range(2):\n",
    "                            for n in range(4):\n",
    "                                if self.Q[i,j,k,l,m,n] > 0:\n",
    "                                    print(i,j,k,l,m,n)\n",
    "\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.22):\n",
    "        self.env = env\n",
    "        self.states = self.get_states()\n",
    "        self.actions = self.get_actions()\n",
    "        self.n_a = len(self.actions)\n",
    "        self.n_s = len(self.states)\n",
    "        self.Q = np.zeros(( len(RANK), len(RANK), len(RANK), len(RANK), len([0,1]), len([0,1,2,3])  ))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for fi in RANK:\n",
    "            for s in RANK:\n",
    "                for t in RANK:\n",
    "                    for fo in RANK:\n",
    "                        state = [\n",
    "                            RANK_VALUE[fi],\n",
    "                            RANK_VALUE[s],\n",
    "                            RANK_VALUE[t],\n",
    "                            RANK_VALUE[fo]\n",
    "                        ]\n",
    "                        states.append(state)\n",
    "        return states\n",
    "\n",
    "    def get_actions(self):\n",
    "        pick = list(range(0, 2))\n",
    "        drop = list(range(0, 4))\n",
    "        actions = []\n",
    "        for p in pick:\n",
    "            for d in drop:\n",
    "                action = [p,d]\n",
    "                actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def epsilon_greed(self, epsilon, s, type):\n",
    "        i0 = s[0] - 1\n",
    "        i1 = s[1] - 1\n",
    "        i2 = s[2] - 1\n",
    "        i3 = s[3] - 1\n",
    "        if type == 'pick':\n",
    "            if np.random.uniform() < epsilon:\n",
    "                index = np.random.randint(2)\n",
    "            else:\n",
    "                index = np.where(self.Q[i0, i1, i2, i3, :, 0] == np.max(self.Q[i0, i1, i2, i3, :, 0]))[0][0]\n",
    "        else:\n",
    "            if np.random.uniform() < epsilon:\n",
    "                index = np.random.randint(4)\n",
    "            else:\n",
    "                index = np.where(self.Q[i0, i1, i2, i3, 0, :] == np.max(self.Q[i0, i1, i2, i3, 0, :]))[0][0]\n",
    "        return index\n",
    "\n",
    "\n",
    "    def train(self, maxiter=10000):\n",
    "        maxiter = maxiter\n",
    "        w=0\n",
    "        l=0\n",
    "        debug = False\n",
    "        for j in range(maxiter):\n",
    "            # self.printQ()\n",
    "            for player in rummy.players:\n",
    "                player.points = player.stash_score()\n",
    "\n",
    "            rummy.reset(rummy.players)\n",
    "            random.shuffle(rummy.players)\n",
    "            # int i = 0\n",
    "            if debug:\n",
    "                print(f'**********************************\\n\\t\\tGame Starts : {j}\\n***********************************')\n",
    "            while not rummy.play():\n",
    "                rummy._update_turn()\n",
    "                if debug:\n",
    "                    print(rummy.max_turns)\n",
    "                for player in rummy.players:\n",
    "                    if player.isBot:\n",
    "                        if rummy.play():\n",
    "                            continue\n",
    "                        if debug:\n",
    "                            print(f'{player.name} Plays')\n",
    "                        rummy.computer_play(player)\n",
    "                        if len(player.stash) == 0:\n",
    "                            l+=1\n",
    "                        if debug:\n",
    "                            player.get_info(debug)\n",
    "                            if len(player.stash) == 0:\n",
    "                                print(f'{player.name} wins the round')\n",
    "\n",
    "                    else:\n",
    "                        if rummy.play():\n",
    "                            continue\n",
    "                        if debug:\n",
    "                            print(f'{player.name} Plays')\n",
    "                        player_info = player.get_info(debug)\n",
    "                        #1s: pick ###################################################################################\n",
    "                        # action_taken = np.random.choice(1)\n",
    "                        epsilon=self.epsilon\n",
    "                        alpha=self.alpha\n",
    "                        gamma=self.gamma\n",
    "                        i0_rank_to_val =player.stash[0].rank_to_val\n",
    "                        i1_rank_to_val =player.stash[1].rank_to_val\n",
    "                        i2_rank_to_val =player.stash[2].rank_to_val\n",
    "                        card_pile_rank_to_val = rummy.pile[-1].rank_to_val\n",
    "                        s = [\n",
    "                            i0_rank_to_val,\n",
    "                            i1_rank_to_val,\n",
    "                            i2_rank_to_val,\n",
    "                            card_pile_rank_to_val\n",
    "                        ]\n",
    "                        a = self.epsilon_greed(epsilon, s, type='pick')\n",
    "                        if debug:\n",
    "                            print(f'Card in pile {player_info[\"PileRank\"]}{player_info[\"PileSuit\"]}')\n",
    "                        if debug:\n",
    "                            print(f'{player.name} takes action {a}')\n",
    "                        result_1 = rummy.pick_card(player, a)\n",
    "                        r1 = result_1[\"reward\"]\n",
    "                        s1 = result_1[\"state\"]\n",
    "                        a1 = self.epsilon_greed(epsilon, s1, type='drop')\n",
    "                        self.Q[s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, a, :] += alpha * (\n",
    "                                r1 + gamma * self.Q[s1[0] - 1, s1[1] - 1, s1[2] - 1, s1[3] - 1, 0, a1] - self.Q[\n",
    "                            s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, a, 0]\n",
    "                        )\n",
    "                        s = s1\n",
    "                        a = a1\n",
    "                        # self.printQ()\n",
    "                        #1e: pick ###################################################################################\n",
    "                        # player stash will have no cards if the player has melded them\n",
    "                        # When you have picked up a card and you have drop it since the remaining cards have been melded.\n",
    "                        if len(player.stash) == 1:\n",
    "                            rummy.drop_card(player, player.stash[0])\n",
    "                            w += 1\n",
    "                            if debug:\n",
    "                                print(f'{player.name} Wins the round')\n",
    "\n",
    "                        elif len(player.stash) != 0:\n",
    "\n",
    "                            player_info = player.get_info(debug)\n",
    "                            if debug:\n",
    "                                print(f'{player.name} takes action {a}')\n",
    "                            # s = player_info['CardRanks']\n",
    "                            #2s: drop ###################################################################################\n",
    "                            # action_taken = np.random.choice(4)\n",
    "                            # s = [player.stash[0].rank_to_val, player.stash[1].rank_to_val, player.stash[2].rank_to_val, player.stash[3].rank_to_val]\n",
    "                            # a = self.epsilon_greed(0.1, s, type='drop')\n",
    "                            card = player.stash[a]\n",
    "                            if debug:\n",
    "                                print(f'{player.name} drops card {card}')\n",
    "                            result_1 = rummy.drop_card(player, card)\n",
    "                            r1 = result_1[\"reward\"]\n",
    "                            card_pile_rank_to_val = rummy.pile[-1].rank_to_val\n",
    "                            s1 = [\n",
    "                                player.stash[0].rank_to_val,\n",
    "                                player.stash[1].rank_to_val,\n",
    "                                player.stash[2].rank_to_val,\n",
    "                                card_pile_rank_to_val\n",
    "                            ]\n",
    "                            a1 = self.epsilon_greed(epsilon, s1, type='pick')\n",
    "                            self.Q[s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, :, a] += alpha * (\n",
    "                                    r1 + gamma * self.Q[s1[0] - 1, s1[1] - 1, s1[2] - 1, s1[3] - 1, a1, 0] -\n",
    "                                    self.Q[s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, 0, a]\n",
    "                            )\n",
    "                            s=s1\n",
    "                            a=a1\n",
    "                            #2e: drop ###################################################################################\n",
    "                        #                             pdb.set_trace()\n",
    "                        else:\n",
    "                            w += 1\n",
    "                            if debug:\n",
    "                                print(f'{player.name} Wins the round')\n",
    "                        if debug:\n",
    "                            player.get_info(debug)\n",
    "            if rummy.max_turns <=0:\n",
    "                score_bot = 0\n",
    "                score_agent = 0\n",
    "                for player in rummy.players:\n",
    "                    if player.isBot:\n",
    "                        score_bot = player.stash_score()\n",
    "                    else:\n",
    "                        score_agent = player.stash_score()\n",
    "                if score_agent < score_bot:\n",
    "                    w+=1\n",
    "                elif score_agent > score_bot:\n",
    "                    l+=1\n",
    "                else:\n",
    "                    w+=0.5\n",
    "                    l+=0.5\n",
    "        # if debug:\n",
    "        print('====================================================', w, l)\n",
    "        return self.Q\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        maxiter = 1\n",
    "        w=0\n",
    "        l=0\n",
    "        debug = False\n",
    "        for j in range(maxiter):\n",
    "            for player in rummy.players:\n",
    "                player.points = player.stash_score()\n",
    "            rummy.reset(rummy.players)\n",
    "            random.shuffle(rummy.players)\n",
    "            if debug:\n",
    "                print(f'**********************************\\n\\t\\t Final Game Starts : \\n***********************************')\n",
    "            while not rummy.play():\n",
    "                rummy._update_turn()\n",
    "                if debug:\n",
    "                    print(rummy.max_turns)\n",
    "                for player in rummy.players:\n",
    "                    if player.isBot:\n",
    "                        if rummy.play():\n",
    "                            continue\n",
    "                        if debug:\n",
    "                            print(f'{player.name} Plays')\n",
    "                        rummy.computer_play(player)\n",
    "                        if len(player.stash) == 0:\n",
    "                            l+=1\n",
    "                        if debug:\n",
    "                            player.get_info(debug)\n",
    "                            if len(player.stash) == 0:\n",
    "                                print(f'{player.name} wins the round')\n",
    "\n",
    "                    else:\n",
    "                        if rummy.play():\n",
    "                            continue\n",
    "                        if debug:\n",
    "                            print(f'{player.name} Plays')\n",
    "                        player_info = player.get_info(debug)\n",
    "                        #1s: pick ###################################################################################\n",
    "                        # action_taken = np.random.choice(1)\n",
    "                        epsilon=0\n",
    "                        i0_rank_to_val =player.stash[0].rank_to_val\n",
    "                        i1_rank_to_val =player.stash[1].rank_to_val\n",
    "                        i2_rank_to_val =player.stash[2].rank_to_val\n",
    "                        card_pile_rank_to_val = rummy.pile[-1].rank_to_val\n",
    "                        s = [\n",
    "                            i0_rank_to_val,\n",
    "                            i1_rank_to_val,\n",
    "                            i2_rank_to_val,\n",
    "                            card_pile_rank_to_val\n",
    "                        ]\n",
    "                        a = self.epsilon_greed(epsilon, s, type='pick')\n",
    "                        if debug:\n",
    "                            print(f'Card in pile {player_info[\"PileRank\"]}{player_info[\"PileSuit\"]}')\n",
    "                        if debug:\n",
    "                            print(f'{player.name} takes action {a}')\n",
    "                        result_1 = rummy.pick_card(player, a)\n",
    "                        # r1 = result_1[\"reward\"]\n",
    "                        s1 = result_1[\"state\"]\n",
    "                        a1 = self.epsilon_greed(epsilon, s1, type='drop')\n",
    "                        # self.Q[s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, a, :] += 0.1 * (\n",
    "                        #         r1 + 0.99 * self.Q[s1[0] - 1, s1[1] - 1, s1[2] - 1, s1[3] - 1, 0, a1] - self.Q[\n",
    "                        #     s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, a, 0]\n",
    "                        # )\n",
    "                        s = s1\n",
    "                        a = a1\n",
    "                        # self.printQ()\n",
    "                        #1e: pick ###################################################################################\n",
    "\n",
    "                        # player stash will have no cards if the player has melded them\n",
    "                        # When you have picked up a card and you have drop it since the remaining cards have been melded.\n",
    "                        if len(player.stash) == 1:\n",
    "                            rummy.drop_card(player, player.stash[0])\n",
    "                            w+=1\n",
    "                            if debug:\n",
    "                                print(f'{player.name} Wins the round')\n",
    "\n",
    "                        elif len(player.stash) != 0:\n",
    "\n",
    "                            player_info = player.get_info(debug)\n",
    "                            if debug:\n",
    "                                print(f'{player.name} takes action {a}')\n",
    "                            # s = player_info['CardRanks']\n",
    "                            #2s: drop ###################################################################################\n",
    "                            # action_taken = np.random.choice(4)\n",
    "                            # s = [player.stash[0].rank_to_val, player.stash[1].rank_to_val, player.stash[2].rank_to_val, player.stash[3].rank_to_val]\n",
    "                            # a = self.epsilon_greed(0.1, s, type='drop')\n",
    "                            card = player.stash[a]\n",
    "                            if debug:\n",
    "                                print(f'{player.name} drops card {card}')\n",
    "                            result_1 = rummy.drop_card(player, card)\n",
    "                            # r1 = result_1[\"reward\"]\n",
    "                            # card_pile_rank_to_val = rummy.pile[-1].rank_to_val\n",
    "                            # s1 = [\n",
    "                            #     player.stash[0].rank_to_val,\n",
    "                            #     player.stash[1].rank_to_val,\n",
    "                            #     player.stash[2].rank_to_val,\n",
    "                            #     card_pile_rank_to_val\n",
    "                            # ]\n",
    "                            # a1 = self.epsilon_greed(0.05, s1, type='pick')\n",
    "                            # self.Q[s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, :, a] += 0.1 * (\n",
    "                            #         r1 + 0.99 * self.Q[s1[0] - 1, s1[1] - 1, s1[2] - 1, s1[3] - 1, a1, 0] -\n",
    "                            #         self.Q[s[0] - 1, s[1] - 1, s[2] - 1, s[3] - 1, 0, a]\n",
    "                            # )\n",
    "                            # s=s1\n",
    "                            # a=a1\n",
    "                            #2e: drop ###################################################################################\n",
    "                        #                             pdb.set_trace()\n",
    "                        else:\n",
    "                            w+=1\n",
    "                            if debug:\n",
    "                                print(f'{player.name} Wins the round')\n",
    "                        if debug:\n",
    "                            player.get_info(debug)\n",
    "            if rummy.max_turns <=0:\n",
    "                score_bot = 0\n",
    "                score_agent = 0\n",
    "                for player in rummy.players:\n",
    "                    if player.isBot:\n",
    "                        score_bot = player.stash_score()\n",
    "                    else:\n",
    "                        score_agent = player.stash_score()\n",
    "                if score_agent < score_bot:\n",
    "                    w+=1\n",
    "                elif score_agent > score_bot:\n",
    "                    l+=1\n",
    "                else:\n",
    "                    w+=0.5\n",
    "                    l+=0.5\n",
    "        return w, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.E Explanation of the codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. __init__**: \n",
    "My RL-agent has an __init__ method to initialize the basic property of this agent. Such as the Rummy gaming environment, the states, the actions, the initialization of my Q-funtion and also different constant such as alpha, gamma, epsilon that i used for the training part.\n",
    "\n",
    "**2. epsilon_greed**: \n",
    "then i defined the epsilon greed method to have some control over the exploration and exploitation. This is basically the default approach for the TD-learning algorithm.\n",
    "\n",
    "\n",
    "**3. train & 4. test**: \n",
    "this 2 methods are basically the one that is training my RL-agent and they are mostly similar. I have implemented the SAARSA algorithm here. \n",
    "\n",
    "so, my Q-function basically is of 6-dimensions : **7 x 7 x 7 x 7 x 2 x 4**\n",
    "\n",
    "so the first 3-dimensions are for the rank of my first 3 card of my stash. Now the 4th dimension varies. Before picking a card, this reflect the \n",
    "\n",
    "rank of the pile-card, but after picking up (whether from deck or pile) this becomes the rank of my 4-th card.\n",
    "\n",
    "**i have basically kept one Q-function for both pick and drop**. so there are 2-extra dimensions in my Q-table. the 5-th one is for picking action, \n",
    "\n",
    "which varies between [0.1] and the 6-th dimension is for the dropping action which contains values of [0,1,2,3] - referring the index of the stash \n",
    "\n",
    "to drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Results\n",
    "\n",
    "### III.A Reports the selected parameters ($\\gamma$, $\\alpha$, and $\\epsilon$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Tune the performance i observed the performance with different combination of alpha, gamma and epsilon.\n",
    "\n",
    "**alpha**: For alpha i have chosen 0.1, 0.2, 0.8 as options.\n",
    "\n",
    "**gamma**: here i used the options 0.8, 0.9, 0.99 values for gamma. \n",
    "\n",
    "**epsilon**: for epsilon the options that i have used are 0.1, 0.25, 0.35. This ensures that we observe something related to almost greedy and \n",
    "\n",
    "also the approach of being somewhat between greedy and random.\n",
    "\n",
    "so i tried with following **27** different combination to observe how they perform on the training, \n",
    "\n",
    "in the training part the number of iteration was equal to **10000** and in the outputs we find the number of times my agent won against the number of time the computer based random bot won, at the time of **training**. \n",
    "\n",
    "**Example (in the following run):** so for param value **[0.1, 0.9, 0.1]** means alpha=0.1, gamma=0.9 and epsilon=0.1 \n",
    "\n",
    "and we if we get the number as i.e. suppose **7935.0** and **2065.0**. This means, out of my **10,000** iteration while training,\n",
    "\n",
    "the implemented RL-agent reached to the goal state **7935** (the first number) times and the computer based random bot reached **2065** (second number on the output) time with these parameters chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================== 7840.0 2160.0\n",
      "Done ===  [0.1, 0.8, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 7541.5 2458.5\n",
      "Done ===  [0.1, 0.8, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 7346.0 2654.0\n",
      "Done ===  [0.1, 0.8, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 7918.0 2082.0\n",
      "Done ===  [0.1, 0.9, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 7553.0 2447.0\n",
      "Done ===  [0.1, 0.9, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 7371.0 2629.0\n",
      "Done ===  [0.1, 0.9, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 7931.5 2068.5\n",
      "Done ===  [0.1, 0.99, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 7532.0 2468.0\n",
      "Done ===  [0.1, 0.99, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 7332.0 2668.0\n",
      "Done ===  [0.1, 0.99, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 7779.5 2220.5\n",
      "Done ===  [0.2, 0.8, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 7376.5 2623.5\n",
      "Done ===  [0.2, 0.8, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 7135.5 2864.5\n",
      "Done ===  [0.2, 0.8, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 7857.5 2142.5\n",
      "Done ===  [0.2, 0.9, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 7431.5 2568.5\n",
      "Done ===  [0.2, 0.9, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 7074.5 2925.5\n",
      "Done ===  [0.2, 0.9, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 7874.5 2125.5\n",
      "Done ===  [0.2, 0.99, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 7393.5 2606.5\n",
      "Done ===  [0.2, 0.99, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 7130.5 2869.5\n",
      "Done ===  [0.2, 0.99, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 6325.5 3674.5\n",
      "Done ===  [0.8, 0.8, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 5934.5 4065.5\n",
      "Done ===  [0.8, 0.8, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 5903.5 4096.5\n",
      "Done ===  [0.8, 0.8, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 6078.5 3921.5\n",
      "Done ===  [0.8, 0.9, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 5774.5 4225.5\n",
      "Done ===  [0.8, 0.9, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 5751.0 4249.0\n",
      "Done ===  [0.8, 0.9, 0.35]\n",
      "\n",
      "\n",
      "==================================================== 5726.0 4274.0\n",
      "Done ===  [0.8, 0.99, 0.1]\n",
      "\n",
      "\n",
      "==================================================== 5482.0 4518.0\n",
      "Done ===  [0.8, 0.99, 0.25]\n",
      "\n",
      "\n",
      "==================================================== 5457.0 4543.0\n",
      "Done ===  [0.8, 0.99, 0.35]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training with different alpha, gamma and epsilon\n",
    "p1 = Player('jawad', list())\n",
    "p2 = Player('comp1', list(), isBot=True)\n",
    "rummy = RummyAgent([p1, p2], max_card_length=3, max_turns=20)\n",
    "\n",
    "param_agent_list = []\n",
    "alpha_gamma_epsilon_list =[]\n",
    "option_alpha_list = [0.1, 0.2, 0.8]\n",
    "option_gamma_list = [0.8, 0.9, 0.99]\n",
    "option_epsilon_list = [0.1, 0.25, 0.35]\n",
    "\n",
    "for alpha in option_alpha_list:\n",
    "    for gamma in option_gamma_list:\n",
    "        for epsilon in option_epsilon_list:\n",
    "            param = [alpha, gamma, epsilon]\n",
    "            alpha_gamma_epsilon_list.append(param)\n",
    "\n",
    "for param in alpha_gamma_epsilon_list:\n",
    "    r = RLAgent(rummy, alpha=param[0], gamma=param[1], epsilon=param[2])\n",
    "    q=r.train(maxiter=10000)\n",
    "    param_agent_list.append([param, r])\n",
    "    print('Done === ', str(param))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B Experimental outputs that show the choice of parameters. How do you choose them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, each time with 10,000 iteration, i have **27** different trained agent with different combination of alpha, gamma and \n",
    "\n",
    "epsilon in the **param_agent_list**. \n",
    "\n",
    "Now, in the following cell, i am going to test each of these trained agent on a test with **1000** iteration. \n",
    "\n",
    "The **test** method does not update the corresponding agent's Q-table in the method, and also i have chosen the epsilon as 0 in the test portion so that the agent takes the greedy option always here in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ===  [0.1, 0.8, 0.1]  ===  806.5\n",
      "Done ===  [0.1, 0.8, 0.25]  ===  854\n",
      "Done ===  [0.1, 0.8, 0.35]  ===  824.5\n",
      "Done ===  [0.1, 0.9, 0.1]  ===  820\n",
      "Done ===  [0.1, 0.9, 0.25]  ===  845\n",
      "Done ===  [0.1, 0.9, 0.35]  ===  810\n",
      "Done ===  [0.1, 0.99, 0.1]  ===  819.0\n",
      "Done ===  [0.1, 0.99, 0.25]  ===  824\n",
      "Done ===  [0.1, 0.99, 0.35]  ===  820.5\n",
      "Done ===  [0.2, 0.8, 0.1]  ===  819.0\n",
      "Done ===  [0.2, 0.8, 0.25]  ===  819\n",
      "Done ===  [0.2, 0.8, 0.35]  ===  823.5\n",
      "Done ===  [0.2, 0.9, 0.1]  ===  824\n",
      "Done ===  [0.2, 0.9, 0.25]  ===  817.5\n",
      "Done ===  [0.2, 0.9, 0.35]  ===  815.0\n",
      "Done ===  [0.2, 0.99, 0.1]  ===  832\n",
      "Done ===  [0.2, 0.99, 0.25]  ===  791.0\n",
      "Done ===  [0.2, 0.99, 0.35]  ===  814.0\n",
      "Done ===  [0.8, 0.8, 0.1]  ===  644.0\n",
      "Done ===  [0.8, 0.8, 0.25]  ===  607.5\n",
      "Done ===  [0.8, 0.8, 0.35]  ===  668.0\n",
      "Done ===  [0.8, 0.9, 0.1]  ===  578.0\n",
      "Done ===  [0.8, 0.9, 0.25]  ===  550.5\n",
      "Done ===  [0.8, 0.9, 0.35]  ===  591.5\n",
      "Done ===  [0.8, 0.99, 0.1]  ===  499.0\n",
      "Done ===  [0.8, 0.99, 0.25]  ===  491.5\n",
      "Done ===  [0.8, 0.99, 0.35]  ===  521.5\n"
     ]
    }
   ],
   "source": [
    "# Testing to find out the param(alpha, gamma, epsilon) based trained agent who performs best\n",
    "param_score_agent_list = []\n",
    "max_score = 0\n",
    "max_param = []\n",
    "max_agent = None\n",
    "for elem in param_agent_list:\n",
    "    param = elem[0]\n",
    "    r = elem[1]\n",
    "    \n",
    "    s=0\n",
    "    for n in range(1000):\n",
    "        w, l = r.test()\n",
    "        s+=w\n",
    "    param_score_agent_list.append([param, s, r])\n",
    "    if s > max_score:\n",
    "        max_score = s\n",
    "        max_param = param\n",
    "        max_agent = r\n",
    "    print('Done === ', str(param), \" === \", str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now i have all of the **27** different agents and their test scores. This scores shows out of the 1000 iteration in the testing portion, how many times the agent reached to the goal state meaning a Win.\n",
    "\n",
    "so, here on these testing experiments, i have stored the agent who performed the best on the test case, and it's score and the parameters based on what it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score:  854\n",
      "Max param (alpha, gamma, epsilon):  [0.1, 0.8, 0.25]\n"
     ]
    }
   ],
   "source": [
    "print('Max score: ', max_score)\n",
    "print('Max param (alpha, gamma, epsilon): ', max_param)\n",
    "# print('Max agent: ', max_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarize i can say that out of all these parameter combinations, i got the best performing agent with parameter\n",
    "alpha = 0.1, gamma=0.8 and epsilon=0.25.\n",
    "\n",
    "Now in the following code, i going to have a simple observation about how this so far best performing agent performs on a test with 10 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max param:  [0.1, 0.8, 0.25]\n",
      "win:  9  vs lose:  1\n"
     ]
    }
   ],
   "source": [
    "# Testing with only one agent (the best so far)\n",
    "w=0\n",
    "l=0\n",
    "for i in range(10):\n",
    "    x, y = max_agent.test()\n",
    "    w+=x\n",
    "    l+=y\n",
    "print('Max param: ', max_param)\n",
    "print('win: ', w, ' vs lose: ', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.C Output plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param:  [0.1, 0.8, 0.1]  vs score:  806.5\n",
      "param:  [0.1, 0.8, 0.25]  vs score:  854\n",
      "param:  [0.1, 0.8, 0.35]  vs score:  824.5\n",
      "param:  [0.1, 0.9, 0.1]  vs score:  820\n",
      "param:  [0.1, 0.9, 0.25]  vs score:  845\n",
      "param:  [0.1, 0.9, 0.35]  vs score:  810\n",
      "param:  [0.1, 0.99, 0.1]  vs score:  819.0\n",
      "param:  [0.1, 0.99, 0.25]  vs score:  824\n",
      "param:  [0.1, 0.99, 0.35]  vs score:  820.5\n",
      "param:  [0.2, 0.8, 0.1]  vs score:  819.0\n",
      "param:  [0.2, 0.8, 0.25]  vs score:  819\n",
      "param:  [0.2, 0.8, 0.35]  vs score:  823.5\n",
      "param:  [0.2, 0.9, 0.1]  vs score:  824\n",
      "param:  [0.2, 0.9, 0.25]  vs score:  817.5\n",
      "param:  [0.2, 0.9, 0.35]  vs score:  815.0\n",
      "param:  [0.2, 0.99, 0.1]  vs score:  832\n",
      "param:  [0.2, 0.99, 0.25]  vs score:  791.0\n",
      "param:  [0.2, 0.99, 0.35]  vs score:  814.0\n",
      "param:  [0.8, 0.8, 0.1]  vs score:  644.0\n",
      "param:  [0.8, 0.8, 0.25]  vs score:  607.5\n",
      "param:  [0.8, 0.8, 0.35]  vs score:  668.0\n",
      "param:  [0.8, 0.9, 0.1]  vs score:  578.0\n",
      "param:  [0.8, 0.9, 0.25]  vs score:  550.5\n",
      "param:  [0.8, 0.9, 0.35]  vs score:  591.5\n",
      "param:  [0.8, 0.99, 0.1]  vs score:  499.0\n",
      "param:  [0.8, 0.99, 0.25]  vs score:  491.5\n",
      "param:  [0.8, 0.99, 0.35]  vs score:  521.5\n"
     ]
    }
   ],
   "source": [
    "for elem in param_score_agent_list:\n",
    "    print('param: ', elem[0], \" vs score: \", elem[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.D reading and analysis of learning results and plots\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Participation in the tournament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Conclusions\n",
    "\n",
    "<!-- Discuss the challenges or somethat that you learned. \n",
    "If you have any suggestion about the assignment, you can write about it.  -->\n",
    "\n",
    "The major challenge that i faced in this assignment is how to define the states and the actions. \n",
    "\n",
    "The algorithm and other approaches are not that hard to implement as from the pseudocode,\n",
    "\n",
    "but without defining the states and the actions properly its quite struggling to go ahead.\n",
    "\n",
    "Another think i guess important to be careful about the states and actions while updating the Q values.\n",
    "\n",
    "One thing i find interesting is to define different reward/penalty based on the different outcomes.\n",
    "\n",
    "It was quite interesting to experiment with different rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit for Tournament Top16!\n",
    "\n",
    "- In two classes, we will have qualification and tournament.\n",
    "- To participate the tournament, you need to use this [client](http://nbviewer.jupyter.org/url/webpages.uncc.edu/mlee173/teach/itcs6156/notebooks/assign/StudentClient.ipynb) codes.\n",
    "- Locally you can run the [server](http://nbviewer.jupyter.org/url/webpages.uncc.edu/mlee173/teach/itcs6156/notebooks/assign/SERVER.ipynb) to verify your run. \n",
    "\n",
    "### Qualification Round \n",
    "\n",
    "- In qualification game, individual will play against a fixed policy agent on the server\n",
    "  - You can access the server: TBA\n",
    "  \n",
    "  - Highly ranked (by the profit) will proceed to the tournament.\n",
    "  - max. 32 players will be cut for main tournament (by 11 pm on Dec 1st).\n",
    "  \n",
    "### Tournament\n",
    "\n",
    "- In tournament, 4 players will be play in each round.\n",
    "  - In a table game, two best scoring players will proceed to next round\n",
    "  - In final, the game will ends in 50 rounds and winner will be the one with lowest balance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "<!-- \n",
    "We will test your notebook with an additional grid.txt file. Please make sure to test in different maze input files. \n",
    " -->\n",
    "points | | description\n",
    "--|--|:--\n",
    "5 | Overview| states the objective and the appraoch \n",
    "35 | Methods | \n",
    " |10| Review of the SARSA and Q-Learning\n",
    " | 5| Choice of TD learning and Reason\n",
    " | 5| Choice of Function Approximation and Reason\n",
    " |10| Implementation of the selected approach (RLAgent)\n",
    " | 5| explanation of the codes\n",
    "45 | Results \n",
    " |10| Reports the selected parameters \n",
    " |15| Experimental outputs that show the choice of parameters. How do you choose them?\n",
    " |10| Output plots (5 for each)\n",
    " |10| reading and analysis of learning results and plots\n",
    "10 | Participation to the tournament.\n",
    "5 | Conclusions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
